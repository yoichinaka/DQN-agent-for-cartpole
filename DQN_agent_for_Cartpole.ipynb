{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN agent for Cartpole",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_kJ44KRBonp",
        "colab_type": "text"
      },
      "source": [
        "COPY from https://keon.io/deep-q-learning/ blog, thank you\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTg54X8sDMnY",
        "colab_type": "text"
      },
      "source": [
        "![代替テキスト](https://keon.io/images/deep-q-learning/deep-q-learning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLwoMmaXDZBG",
        "colab_type": "text"
      },
      "source": [
        "target = reward + gamma * np.amax(model.predict(next_state))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9ruF72gDoT0",
        "colab_type": "text"
      },
      "source": [
        "## **Remember**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> One of the challenges for DQN is that neural network used in the algorithm tends to forget the previous experiences as it overwrites them with new experiences. So we need a list of previous experiences and observations to re-train the model with the previous experiences. We will call this array of experiences memory and use remember() function to append state, action, reward, and next state to the memory.\n",
        "\n",
        "\n",
        "\n",
        "In our example, the memory list will have a form of:\n",
        "\n",
        "\n",
        "> memory = [(state, action, reward, next_state, done)...]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "And remember function will simply store states, actions and resulting rewards to the memory like below:\n",
        "\n",
        "\n",
        "\n",
        "> def remember(self, state, action, reward, next_state, done):\n",
        "\n",
        "\n",
        "\n",
        "> self.memory.append((state, action, reward, next_state, done))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1xZecvRD9KZ",
        "colab_type": "text"
      },
      "source": [
        "# Replay\n",
        "\n",
        "A method that trains the neural net with experiences in the memory is called replay(). First, we sample some experiences from the memory and call them minibath.\n",
        "\n",
        "\n",
        "> minibatch = random.sample(self.memory, batch_size)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgDBq-O_EiZ_",
        "colab_type": "text"
      },
      "source": [
        "## **How The Agent Decides to Act**\n",
        "\n",
        "Our agent will randomly select its action at first by a certain percentage, called ‘exploration rate’ or ‘epsilon’. This is because at first, it is better for the agent to try all kinds of things before it starts to see the patterns. When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. np.argmax() is the function that picks the highest value between two elements in the act_values[0]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvaStN91EoiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vowgbaz27DgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP8bhnQG8I7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGdGqBHK73KG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tshOI3Vo8Oqp",
        "colab_type": "code",
        "outputId": "8504fb2f-2631-496d-a769-13bed7172318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "agent = DQNAgent(state_size, action_size)\n",
        "# agent.load(\"./save/cartpole-dqn.h5\")\n",
        "done = False\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpyeobc88tJr",
        "colab_type": "code",
        "outputId": "1d55346e-c41e-4e98-fe70-07cd26d2e434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPISODES = 1000\n",
        "for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(500):\n",
        "            # env.render()\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "        # if e % 10 == 0:\n",
        "        #     agent.save(\"./save/cartpole-dqn.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 0/1000, score: 13, e: 1.0\n",
            "episode: 1/1000, score: 10, e: 1.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "episode: 2/1000, score: 21, e: 0.93\n",
            "episode: 3/1000, score: 21, e: 0.84\n",
            "episode: 4/1000, score: 19, e: 0.76\n",
            "episode: 5/1000, score: 21, e: 0.69\n",
            "episode: 6/1000, score: 16, e: 0.63\n",
            "episode: 7/1000, score: 14, e: 0.59\n",
            "episode: 8/1000, score: 18, e: 0.54\n",
            "episode: 9/1000, score: 21, e: 0.49\n",
            "episode: 10/1000, score: 11, e: 0.46\n",
            "episode: 11/1000, score: 15, e: 0.43\n",
            "episode: 12/1000, score: 14, e: 0.4\n",
            "episode: 13/1000, score: 10, e: 0.38\n",
            "episode: 14/1000, score: 12, e: 0.36\n",
            "episode: 15/1000, score: 9, e: 0.34\n",
            "episode: 16/1000, score: 13, e: 0.32\n",
            "episode: 17/1000, score: 10, e: 0.3\n",
            "episode: 18/1000, score: 9, e: 0.29\n",
            "episode: 19/1000, score: 7, e: 0.28\n",
            "episode: 20/1000, score: 17, e: 0.26\n",
            "episode: 21/1000, score: 9, e: 0.25\n",
            "episode: 22/1000, score: 9, e: 0.23\n",
            "episode: 23/1000, score: 9, e: 0.22\n",
            "episode: 24/1000, score: 10, e: 0.21\n",
            "episode: 25/1000, score: 8, e: 0.21\n",
            "episode: 26/1000, score: 9, e: 0.2\n",
            "episode: 27/1000, score: 9, e: 0.19\n",
            "episode: 28/1000, score: 8, e: 0.18\n",
            "episode: 29/1000, score: 14, e: 0.17\n",
            "episode: 30/1000, score: 11, e: 0.16\n",
            "episode: 31/1000, score: 8, e: 0.15\n",
            "episode: 32/1000, score: 8, e: 0.15\n",
            "episode: 33/1000, score: 9, e: 0.14\n",
            "episode: 34/1000, score: 7, e: 0.14\n",
            "episode: 35/1000, score: 10, e: 0.13\n",
            "episode: 36/1000, score: 7, e: 0.12\n",
            "episode: 37/1000, score: 9, e: 0.12\n",
            "episode: 38/1000, score: 10, e: 0.11\n",
            "episode: 39/1000, score: 9, e: 0.11\n",
            "episode: 40/1000, score: 8, e: 0.1\n",
            "episode: 41/1000, score: 10, e: 0.099\n",
            "episode: 42/1000, score: 8, e: 0.095\n",
            "episode: 43/1000, score: 9, e: 0.091\n",
            "episode: 44/1000, score: 7, e: 0.088\n",
            "episode: 45/1000, score: 9, e: 0.084\n",
            "episode: 46/1000, score: 10, e: 0.08\n",
            "episode: 47/1000, score: 9, e: 0.076\n",
            "episode: 48/1000, score: 9, e: 0.073\n",
            "episode: 49/1000, score: 10, e: 0.069\n",
            "episode: 50/1000, score: 24, e: 0.061\n",
            "episode: 51/1000, score: 12, e: 0.058\n",
            "episode: 52/1000, score: 30, e: 0.05\n",
            "episode: 53/1000, score: 26, e: 0.044\n",
            "episode: 54/1000, score: 40, e: 0.036\n",
            "episode: 55/1000, score: 61, e: 0.026\n",
            "episode: 56/1000, score: 38, e: 0.022\n",
            "episode: 57/1000, score: 19, e: 0.02\n",
            "episode: 58/1000, score: 55, e: 0.015\n",
            "episode: 59/1000, score: 32, e: 0.013\n",
            "episode: 60/1000, score: 27, e: 0.011\n",
            "episode: 61/1000, score: 61, e: 0.01\n",
            "episode: 62/1000, score: 38, e: 0.01\n",
            "episode: 63/1000, score: 35, e: 0.01\n",
            "episode: 64/1000, score: 26, e: 0.01\n",
            "episode: 65/1000, score: 47, e: 0.01\n",
            "episode: 66/1000, score: 39, e: 0.01\n",
            "episode: 67/1000, score: 41, e: 0.01\n",
            "episode: 68/1000, score: 61, e: 0.01\n",
            "episode: 69/1000, score: 34, e: 0.01\n",
            "episode: 70/1000, score: 33, e: 0.01\n",
            "episode: 71/1000, score: 30, e: 0.01\n",
            "episode: 72/1000, score: 56, e: 0.01\n",
            "episode: 73/1000, score: 92, e: 0.01\n",
            "episode: 74/1000, score: 63, e: 0.01\n",
            "episode: 75/1000, score: 71, e: 0.01\n",
            "episode: 76/1000, score: 143, e: 0.01\n",
            "episode: 77/1000, score: 94, e: 0.01\n",
            "episode: 78/1000, score: 146, e: 0.01\n",
            "episode: 79/1000, score: 144, e: 0.01\n",
            "episode: 80/1000, score: 193, e: 0.01\n",
            "episode: 81/1000, score: 114, e: 0.01\n",
            "episode: 82/1000, score: 167, e: 0.01\n",
            "episode: 83/1000, score: 120, e: 0.01\n",
            "episode: 84/1000, score: 131, e: 0.01\n",
            "episode: 85/1000, score: 212, e: 0.01\n",
            "episode: 86/1000, score: 207, e: 0.01\n",
            "episode: 87/1000, score: 9, e: 0.01\n",
            "episode: 88/1000, score: 9, e: 0.01\n",
            "episode: 89/1000, score: 207, e: 0.01\n",
            "episode: 90/1000, score: 209, e: 0.01\n",
            "episode: 91/1000, score: 194, e: 0.01\n",
            "episode: 92/1000, score: 499, e: 0.01\n",
            "episode: 93/1000, score: 232, e: 0.01\n",
            "episode: 94/1000, score: 214, e: 0.01\n",
            "episode: 95/1000, score: 253, e: 0.01\n",
            "episode: 96/1000, score: 204, e: 0.01\n",
            "episode: 97/1000, score: 157, e: 0.01\n",
            "episode: 98/1000, score: 251, e: 0.01\n",
            "episode: 99/1000, score: 235, e: 0.01\n",
            "episode: 100/1000, score: 181, e: 0.01\n",
            "episode: 101/1000, score: 199, e: 0.01\n",
            "episode: 102/1000, score: 182, e: 0.01\n",
            "episode: 103/1000, score: 16, e: 0.01\n",
            "episode: 104/1000, score: 39, e: 0.01\n",
            "episode: 105/1000, score: 208, e: 0.01\n",
            "episode: 106/1000, score: 53, e: 0.01\n",
            "episode: 107/1000, score: 213, e: 0.01\n",
            "episode: 108/1000, score: 209, e: 0.01\n",
            "episode: 109/1000, score: 214, e: 0.01\n",
            "episode: 110/1000, score: 254, e: 0.01\n",
            "episode: 111/1000, score: 245, e: 0.01\n",
            "episode: 112/1000, score: 487, e: 0.01\n",
            "episode: 113/1000, score: 230, e: 0.01\n",
            "episode: 114/1000, score: 175, e: 0.01\n",
            "episode: 115/1000, score: 228, e: 0.01\n",
            "episode: 116/1000, score: 214, e: 0.01\n",
            "episode: 117/1000, score: 35, e: 0.01\n",
            "episode: 118/1000, score: 11, e: 0.01\n",
            "episode: 119/1000, score: 235, e: 0.01\n",
            "episode: 120/1000, score: 246, e: 0.01\n",
            "episode: 121/1000, score: 32, e: 0.01\n",
            "episode: 122/1000, score: 162, e: 0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-75f50c7fcae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m# if e % 10 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#     agent.save(\"./save/cartpole-dqn.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-de12c9705614>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtarget_f\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mindex_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.shuffle\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, array, ptr)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;31m# get a void pointer to the buffer, which keeps the array alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_void_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_internal.py\u001b[0m in \u001b[0;36m_get_void_ptr\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;31m# finally cast to void*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ctypes/__init__.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(obj, typ)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0m_cast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPYFUNCTYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_void_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cast_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0m_string_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPYFUNCTYPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_void_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_string_at_addr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}